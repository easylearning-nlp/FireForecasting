# 模型详解

## Vision Transformer模型详解

模型的整体架构图如下所示。简单而言，由三个模块组成，分别如下：

（1）Linear Projection of Flattened patches（Embedding层）

（2）Transformer Encoder

（3）MLP Head（这是由于作者在VIT一文中主要针对图像分类）

![img](https://pic3.zhimg.com/80/v2-5a9edbca315c2c96dad2c7fb28e40632_1440w.png)

## Embedding层结构详解

对于标准的Transformer模块而言，通常要求输入的是token（向量）序列，即二维矩阵[num_token, token_dim]，可以理解为输入句子的单词个数和词向量维度，如下图所示，token0-9对应的都是向量，以本文提到的ViT-B/16为例子，每个token向量的长度都是768。

<img src="/Users/easylearninghow/Desktop/ViT/TE.png" alt="image-20220413112901901" style="zoom: 50%;" />

对于图像数据而言，其数据格式为[H, W, C]，分别代表的是图片的通道数Channel，图片的高Height和宽Width。但很明显的是三维数据并不是Transformer所需要的。所以需要通过使用一个Embedding层来对原始的图片数据进行变换。如下图所示，首先将给定的一堆图片按照给定的大小分成一堆Patches。本文将输入的图片(224×224)按照16×16大小的Patch进行划分。其中（224×224）/（16×16）=196，因此我们会得到196个patches。到这里我们可以知道每一个Patches数据的shape为[16, 16, 3]。为了满足Transformer的需求，在这里，对每个Patch进行投影变化，映射到一维向量中。即完成如下转化。[16, 16, 3]->[768]，那么这样一来，就将原始的[224, 224, 3]转化为[196, 768]。

<img src="/Users/easylearninghow/Library/Application Support/typora-user-images/image-20220413132610833.png" alt="image-20220413132610833" style="zoom:50%;" />

在代码中，其实只需要使用一个卷积层就可以完成这个操作。使用卷积核（kernel_size）大小为（16×16），步距（stride）为16，卷积核个数为768的卷积来实现。通过卷积操作，将维度[224, 224, 3]转化为[14, 14, 768]，然后将H，W这两个维度进行展平即可。即[14, 14, 768]->[196, 768]，此时就转换成了二维矩阵，可以理解为将一张图片，变化成了196个token，其中每个token的维度为768。很显然，这正是Transformer所期望的数据维度。

在输入Transformer Encoder之前，值得注意的是需要加上[class] token以及Position Embedding。在原论文中，作者的意思是参考BERT，在上述得到的一堆tokens中插入一个专门用于分类操作的[class] token，这个[class] token是一个可训练的参数，数据格式和其他token保持一致，均为一个向量。以本文为例，其维度大小为[1, 768]。注意的是，这里采取的是Concat操作。即cat([1, 768], [196, 768]) -> [197, 768]，此时正好变成了二维矩阵。接着就是关于如何加入Position Embedding就是之前Transformer中讲到的Position Encoding，文章提到的Position Embedding使用的是一个可训练的参数，其实在Attention is all you need中提到的是一个正余弦固定向量作为Position Embedding，至于哪一个效果更好，不妨自己做一下实验测试测试。上述维度是[197, 768]，因此这里生成的Position Embedding维度也是[197, 768]。值得注意的是这里采用是直接叠加在tokens上。因此最终的维度是add([197, 768], [197, 768]) = [197, 768]。代码如下：

```python
class PatchEMbedding(nn.Module):
    def __init__(self, img_size=224, patch_size=16, in_channel=3, embed_dim=768, norm_layer=None):
        super().__init__()
        img_size = (img_size, img_size)
        patch_size = (patch_size, patch_size)
        self.img_size = img_size
        self.patch_size = patch_size
        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])
        self.num_patches = self.grid_size[0] * self.grid_size[1]
        
        self.projection = nn.Conv2d(in_channel, 
                                    embed_dim,
                                   kernel_size=patch_size,
                                   stride=patch_size)
        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()
        
    def forward(self, x):
        '''
        [Batch_size, Channel, Height, Width]
        flatten:[B, C, H, W] ->[B, C, HW]
        transpose:[B, C, HW] ->[B, HW, C]
        '''
        B, C, H, W = x.shape
        assert H == self.img_size[0] and W == self.img_size[1], \
            f"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]})."
        x = self.projection(x).flatten(2).transpose(1,2)
        x = self.norm(x)
        return x
```

## Transformer Encoder详解

Transformer Encoder其实就是重复堆叠Encoder Bolock L次，如下图所示：

<img src="/Users/easylearninghow/Library/Application Support/typora-user-images/image-20220413133202846.png" alt="image-20220413133202846" style="zoom:50%;" />

（1）Layer Norm，这种Normalization方法针对NLP领域提出的，在ViT中是针对每个Token进行Norm处理的，后续会出一期博客专门介绍Layer Norm和Batch Norm的区别。

（2）Muti-Head Attention，来自Nips2017的Attention is all you need。

（3）MLP，多层感知机，其实就是全连接层+GELU激活函数+Dropout。值得注意的是，第一个全连接层会把输入token维度放大四倍[197, 768]->[197, 3012]，然后会经过投影变化为[197, 3012]->[197, 768]。笔者花了一个详细的维度变化图，如下所示。很明显开始是[197, 768]，设置的head个数是12，维度变化为[197, 64]，紧接着做注意力机制计算，计算后维度依旧是[197, 64]，然后对所有的head进行Concat操作后的维度是[197, 768]，经过MLP开始的时候放大4倍，变化为[197, 3012]，随后经过投影变化为[197, 768]。这里也不难看出为什么Transformer Encoder模块可以不断堆叠，因为输入维度和输出维度是不变的。

<img src="/Users/easylearninghow/Library/Application Support/typora-user-images/image-20220413140750395.png" alt="image-20220413140750395" style="zoom:50%;" />

代码如下：

```python
class Attention(nn.Module):
    def __init__(self,
                token_dim=768,
                num_heads=12,
                qkv_bias=False,
                qk_scale=None,
                attn_drop_ration=0.,
                proj_drop_ratio=0.):
        super(Attention, self).__init__()
        self.num_heads=num_heads
        head_dim = token_dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5
        self.qkv = nn.Linear(token_dim, token_dim*3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop_ration)
        self.proj = nn.Linear(token_dim, token_dim)
        self.proj_drop = nn.Dropout(proj_drop_ratio)
    
    def forward(self, x):
        '''
        [batch_size, num_patches+1, total_embed_dim]->[B,197,768]
        qkv(): -> [batch_size, num_patches+1, 3*total_embed_dim]->[B,197,2304]
        reshape: -> [batch_size, num_patches+1, 3, num_heads, embed_dim_per_head] ->[B,197,3,12,64]
        permute: -> [3, batch_size, num_heads, num_patches+1, embed_dim_per_head] ->[3,B,12,197,64]
        '''
        B, N, C = x.shape
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C//self.num_heads).permute(2,0,3,1,4)
        q,k,v = qkv[0], qkv[1], qkv[2] #[batch_size, num_heads, num_patches + 1, embed_dim_per_head]
        '''
        transpose: ->[batch_size, num_heads, embed_dim_per_head, num_patches + 1]
        @: multiply -> [batch_size, num_heads, num_patches + 1, num_patches + 1]
        '''
        attention_map = (q @ k.transpose(-2,-1))*self.scale
        attention_map = attention_map.softmax(dim=-1)
        attention_map = self.attn_drop(attention_map)
        '''
        @: multiply -> [batch_size, num_heads, num_patches + 1, embed_dim_per_head]
        transpose: -> [batch_size, num_patches+1, num_heads, embed_dim_per_head]
        reshape: -> [batch_size, num_patches+1, total_embed_dim]
        '''
        x = (attention_map @ v).transpose(1,2).reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x
```

## MLP Head

上面经过了Transformer Encoder之后输出的shape和输入的shape是不变的，笔者在这里写了一个简单的测试代码。

```python
PE = PatchEMbedding()
x = torch.randn([32, 3, 224, 224])
print('原始输入维度',x.shape)
x = PE(x)
print('PatchEmbedding后的维度',x.shape)
cls_token = nn.Parameter(torch.zeros(32, 1, 768))
print(cls_token.shape)
x = torch.cat((cls_token, x), dim=1)
print('加上了class token',x.shape)
AE = Attention()
am  = AE(x)
print('做注意力机制后的维度',am.shape)
```

```markdown
原始输入维度 torch.Size([32, 3, 224, 224])
PatchEmbedding后的维度 torch.Size([32, 196, 768])
torch.Size([32, 1, 768])
加上了class token torch.Size([32, 197, 768])
做注意力机制后的维度 torch.Size([32, 197, 768])
```

得出结果如上所示。输入[197, 768]输出也是[197, 768]。其中ViT是用来做分类任务，所以我们只需要提取出[class]token生成对应的结果就行。即在[197, 768]中抽取[class] token对应的[1, 768]。接着我们通过MLP Head得到我们最终的分类结果。原文使用的是Linear+Tanh激活函数+Linear。但是针对简单数据而言，只用Linear即可。

<img src="/Users/easylearninghow/Library/Application Support/typora-user-images/image-20220413152517404.png" alt="image-20220413152517404" style="zoom: 25%;" />

```python
class Mlp(nn.Module):
    def __init__(self, 
                 in_features, 
                 hidden_features=None, 
                 out_features=None, 
                 act_layer=nn.GELU, 
                 drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)
    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x
```

## TransformerBlock           

关于Transformer Block的模块架构在Transformer Block详解部分已经描述了，在构建了所有模块以后，笔者这里按照一层一层的堆叠书写了下述代码。

```python
class TransformerEncoderBlock(nn.Module):
    def __init__(self,
                token_dim=768,
                num_heads=12,
                mlp_ratio=4.,
                qkv_bias=False,
                qk_scale=None,
                drop_ratio=0.,
                attn_drop_ratio=0.,
                drop_path_ratio=0.,
                act_layer=nn.GELU,
                norm_layer=nn.LayerNorm):
        super(TransformerEncoderBlock, self).__init__()
        self.norm1 = norm_layer(token_dim)
        self.attention_map = Attention(token_dim, 
                                       num_heads=num_heads,
                                       qkv_bias=qkv_bias,
                                       qk_scale=qk_scale,
                                       attn_drop_ratio=attn_drop_ratio,
                                       proj_drop_ratio=drop_ratio)
        self.drop_path = DropPath(drop_path_ratio) if drop_path_ratio > 0. else nn.Identity()
        self.norm2 = norm_layer(token_dim)
        mlp_hidden_dim = int(token_dim*mlp_ratio)
        self.mlp = Mlp(in_features=token_dim, 
                       hidden_features=mlp_hidden_dim,
                       act_layer=act_layer,
                       drop=drop_ratio)
    def forward(self, x):
        x = x + self.drop_path(self.attention_map(self.norm1(x)))
        x = x + self.drop_path(self.mlp(self.norm2(x)))
        return x
```

## Vision Transformer模块搭建

笔者在这里将Vision Transformer的网络架构大致花了出来，其实模块如何搭建已经在上文展示过了，现在就是如何来进行组装堆叠。接下来，直接阅读代码，仔细看懂就能弄明白是如何搭建ViT的。代码如下。



<img src="/Users/easylearninghow/Library/Application Support/typora-user-images/image-20220413184053394.png" alt="image-20220413184053394"  />

```python
class VisionTransformer(nn.Module):
    def __init__(self, img_size=224, patch_size=16, in_c=3, num_classes=1000,
                 embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0, qkv_bias=True,
                 qk_scale=None, representation_size=None, distilled=False, drop_ratio=0.,
                 attn_drop_ratio=0., drop_path_ratio=0., embed_layer=PatchEMbedding, norm_layer=None,
                 act_layer=None):
        super(VisionTransformer, self).__init__()
        self.num_classes = num_classes
        self.num_features = self.embed_dim = embed_dim
        self.num_tokens = 2 if distilled else 1
        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)
        act_layer = act_layer or nn.GELU
        self.patch_embed = embed_layer(img_size=img_size, 
                                       patch_size=patch_size, 
                                       in_c=in_c, 
                                       embed_dim=embed_dim)
        num_patches = self.patch_embed.num_patches
        
        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        self.dist_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) if distilled else None
        self.pos_embedding = nn.Parameter(torch.zeros(1, num_patches+self.num_tokens, embed_dim))
        self.pos_drop = nn.Dropout(p=drop_ratio)
        
        dpr = [x.item() for x in torch.linspace(0, drop_path_ratio, depth)]
        self.blocks = nn.Sequential(*[
            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratiop, qkv_bias=qkv_bias,
                 qk_scale=qk_scale,drop_ratio=drop_ratio, attn_drop_ratio=attn_drop_ratio, 
                 drop_path_ratio=dpr[i],
                 norm_layer=norm_layer, act_layer=act_layer)
            for i in range(depth)
        ])
        self.norm = norm_layer(embed_dim)
        
        # Representation layer
        if representation_size and not distilled:
            self.has_logits = True
            self.num_features = representation_size
            self.pre_logits = nn.Sequential(OrderedDict([
                ("fc", nn.Linear(embed_dim, representation_size)),
                ("act", nn.Tanh())
            ]))
        else:
            self.has_logits = False
            self.pre_logits = nn.Identity()

        # Classifier head(s)
        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
        self.head_dist = None
        if distilled:
            self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()

        # Weight init
        nn.init.trunc_normal_(self.pos_embed, std=0.02)
        if self.dist_token is not None:
            nn.init.trunc_normal_(self.dist_token, std=0.02)

        nn.init.trunc_normal_(self.cls_token, std=0.02)
        self.apply(_init_vit_weights)
    
    def forward_features(self, x):
        #[B,C,H,W] ->[B,num_patches, embed_dim]
        x = self.patch_embed(x) #[B, 196, 768]
        cls_token = self.cls_token.expand(x.shape[0],-1,-1)#[1,1,768]->[B,1,768]
        x = torch.cat((cls_token, x), dim=1) #[B,197,768]
        x = self.pos_drop(x+self.pos_embedding)
        x = self.blocks(x)
        x = self.norm(x)
    def forward(self, x):
        x = self.forward_features(x)
        return x
```

因此整个ViT的搭建如上所示。ViT模型的搭建也就完成了。初次执笔，有疑惑或者异议的可以留言讨论。